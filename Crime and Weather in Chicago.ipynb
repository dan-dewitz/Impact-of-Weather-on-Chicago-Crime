{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Effect of Weather and Seasonality on Crime in Chicago\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "Chicago, the windy city, has brutal, cold winters and hot, humid summers. Chicago is also know for it's high rate of crime and murder. The aim of this project is to build a data pipeline and framework that makes it easy for a data science team to quickly load clean data with an intuitive schema that makes is easy to run statistical models and machine learning algorithms on Spark.\n",
    "\n",
    "The project contains the following modules:\n",
    "* Scope of the Project Data\n",
    "    - where the data comes from and tools\n",
    "* Project Setup\n",
    "    - required libraries, environment setup\n",
    "* Explore and Assess the data\n",
    "    - identifying data quality issues\n",
    "* Defining the Data Model\n",
    "    - ETL, Schema, Data quality checks\n",
    "* Data dictionary\n",
    "* Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope of the Project and Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "To analyze the effects of weather and seasonality on crime in Chicago, I need at least two data sources -- one containing crime data for Chicago and one containing weather data for Chicago.     \n",
    "\n",
    "The city of Chicago publishes crime data and makes it publically avalibile on their [website](https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-present/ijzp-q8t2).    \n",
    "\n",
    "For the weather data, I chose [NOAA](https://www.noaa.gov), as it is a standard source for weather data that a lot of people in the data science community are familiar with.     \n",
    "\n",
    "The primary aim of this data pipeline is to wrangle these two data sources into a schema that is intuitive and easy to query to perform timeseries and statistical modeling.     \n",
    "\n",
    "Because the primary goal is to model the data, the tool I am using is Spark. I chose Spark because the wrangling and modeling can all be done using the same tool. And because the statistical models will likely be computationally intensive, and therefore, a distributed data archateture is needed.\n",
    "\n",
    "\n",
    "#### Data      \n",
    "\n",
    "`crime data`\n",
    "* the crime data is from the City of Chicago website located [here](https://data.cityofchicago.org/Public-Safety/Crimes-2001-to-present/ijzp-q8t2)\n",
    "* the dates range from 2001 to present\n",
    "* 6,975,703 total rows in csv export     \n",
    "\n",
    "`weather data`\n",
    "* the weather data is from [NOAA](https://www.noaa.gov)\n",
    "* the dates range from 2001 to present\n",
    "* 6,836 total rows\n",
    "\n",
    "`IUCR data` \n",
    "* IUCR data is located [here](https://data.cityofchicago.org/Public-Safety/Chicago-Police-Department-Illinois-Uniform-Crime-R/c7ck-438e)\n",
    "* standard code for reporting type of crime\n",
    "* 401 total rows in csv export\n",
    "\n",
    "\n",
    "#### Tools\n",
    "All data wrangling and analyses are done in *Spark*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Required Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get AWS credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['KEY']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['KEY']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore and Assess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Crime Data | Part 2 - Weather Data | Part 3 - IUCR Data\n",
    "\n",
    "### Crime Data\n",
    "\n",
    "After evaluating the crime data and schema, I am primarily interested in the following variables, and need to perform the following data wrangling steps: \n",
    "* ID\n",
    "    - A unique crime event. \n",
    "    - I want to verify that this in the primary key of the raw data.  \n",
    "* Date\n",
    "    - Using date is a greate way to distill seasonal patterns. \n",
    "    - This value should never be null.\n",
    "    - This value needs to be formated from 01/01/2019 to 2019-01-01\n",
    "* IUCR\n",
    "    - Universal code that distinguishes type of crime. For example, assault, theft, burglary, etc. \n",
    "    - Potentially, certain types of crimes will have seasonal patterns, while other types of crime may not\n",
    "    - I do not want to include records with a null IUCR code\n",
    "* Domestic\n",
    "    - Indicates if crime was committed in a domestic setting or not. \n",
    "    - It is possible that crimes that occure at a residence will be less impacted by storm and seasonal effects of weather\n",
    "    - Null values can be kept, and excluded if needed for specific analyses\n",
    "* Community Area\n",
    "    - It is possible that in certain areas of Chicago crime follows seasonal patterns, while it other parts of the city, it does not\n",
    "    - Null values can be kept, and excluded if needed for specific analyses\n",
    "    - coalesce() null values to 'no_community_reported'\n",
    "* Crime Count\n",
    "    - How many crimes occured on a given day, in a specific community, and the specific type of crime that occured\n",
    "    - A distinct count of the ID column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in data and show the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the crime data\n",
    "crime_raw = spark.read.csv(\"s3a://chicago-crime-ddd/Chicago_Crime_2001_to_Present.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create crime_raw view\n",
    "crime_raw.createOrReplaceTempView(\"crime_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select variables of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_small = spark.sql('''\n",
    "                        SELECT\n",
    "                             ID,\n",
    "                             Date,\n",
    "                             IUCR,\n",
    "                             Domestic,\n",
    "                             `Community Area`,\n",
    "                             `Case Number`\n",
    "                        FROM crime_raw                          \n",
    "                        ORDER BY DATE\n",
    "                        ''')\n",
    "\n",
    "# create crime_small view\n",
    "crime_small.createOrReplaceTempView(\"crime_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- IUCR: string (nullable = true)\n",
      " |-- Domestic: string (nullable = true)\n",
      " |-- Community Area: string (nullable = true)\n",
      " |-- Case Number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the schema\n",
    "crime_small.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine primary key\n",
    "##### Is the 'ID' colume truely the primary key?    \n",
    "* Yes, the ID is the primary key. But Case Numbers can have multiple records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------------+\n",
      "|row_count|ID_dist_cnt|CASE_NUM_dist_cnt|\n",
      "+---------+-----------+-----------------+\n",
      "|  6975703|    6975703|          6975280|\n",
      "+---------+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check that row count is the ID column is indeed the Primary Key\n",
    "# a very small number of Case Numbers have multiple records. \n",
    "spark.sql('''\n",
    "          SELECT COUNT(*) AS row_count,\n",
    "                 COUNT(distinct ID) as ID_dist_cnt,\n",
    "                 COUNT(distinct `Case Number`) as CASE_NUM_dist_cnt\n",
    "          FROM crime_small\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Find the Case Numbers with the most records\n",
    "* since the max records for a Case Number is 6, this is not a data quality concern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|case_num|row_count|\n",
      "+--------+---------+\n",
      "|HZ140230|        6|\n",
      "|HJ590004|        6|\n",
      "|HS256531|        5|\n",
      "|HP296582|        5|\n",
      "|HJ104730|        4|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          SELECT DISTINCT\n",
    "             `Case Number` as case_num,\n",
    "              COUNT(*) as row_count\n",
    "          FROM crime_small          \n",
    "          GROUP BY 1\n",
    "          HAVING row_count > 1\n",
    "          ORDER BY row_count DESC\n",
    "          LIMIT 5\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Look at all records for the Case Numbers with 6 records each\n",
    "* records are written to s3 for easy viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I know that Case Number HZ140230 and HJ590004 have the most records, with 6 records each. So I want to take a look and see what is going on\n",
    "test = spark.sql('''\n",
    "          SELECT *\n",
    "          FROM crime_small          \n",
    "          WHERE `Case Number` IN ('HZ140230', 'HJ590004')\n",
    "          ''')\n",
    "\n",
    "# convert to pandas df\n",
    "test_pd = test.toPandas()\n",
    "\n",
    "# write to csv on s3\n",
    "test.write.csv(os.path.join(\"s3a://chicago-crime-ddd/\", 'test'), 'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check for null values in Case Number\n",
    "* four values are null; since it is so few lines are null, I am comfortable throwing them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|case_num_null_status|row_count|\n",
      "+--------------------+---------+\n",
      "|                good|  6975699|\n",
      "|            bad_null|        4|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          SELECT DISTINCT\n",
    "              CASE\n",
    "                  WHEN `Case Number` IS NULL THEN 'bad_null'\n",
    "                  WHEN `Case Number` IS NOT NULL THEN 'good'\n",
    "                  ELSE 'weird'\n",
    "              END AS case_num_null_status,    \n",
    "              COUNT(*) as row_count\n",
    "          FROM crime_small          \n",
    "          GROUP BY 1\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formate Date\n",
    "##### Test and verify date formating functions\n",
    "Format date to standard database form ('yyyy-MM-dd')    \n",
    "The date from the crime date and the date from the weather data must be the same data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+--------+----------+----------+\n",
      "|                Date|sub_month|sub_day|sub_year| good_date|good_month|\n",
      "+--------------------+---------+-------+--------+----------+----------+\n",
      "|01/01/2001 01:00:...|       01|     01|    2001|2001-01-01|         1|\n",
      "|01/01/2001 01:00:...|       01|     01|    2001|2001-01-01|         1|\n",
      "|01/01/2001 01:00:...|       01|     01|    2001|2001-01-01|         1|\n",
      "|01/01/2001 01:01:...|       01|     01|    2001|2001-01-01|         1|\n",
      "|01/01/2001 01:02:...|       01|     01|    2001|2001-01-01|         1|\n",
      "+--------------------+---------+-------+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          SELECT DISTINCT \n",
    "              Date,\n",
    "              substr(Date, 1, 2) AS sub_month,\n",
    "              substr(Date, 4, 2) AS sub_day,\n",
    "              substr(Date, 7, 4) AS sub_year,\n",
    "              to_date(concat(substr(Date, 7, 4), '-', substr(Date, 1, 2), '-', substr(Date, 4, 2)), 'yyyy-MM-dd') AS good_date,\n",
    "              month(to_date(concat(substr(Date, 7, 4), '-', substr(Date, 1, 2), '-', substr(Date, 4, 2)), 'yyyy-MM-dd')) AS good_month\n",
    "          FROM crime_small\n",
    "          LIMIT 5\n",
    "          ''').show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check for null values in Date\n",
    "* no null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+\n",
      "|Date_null_status|row_count|\n",
      "+----------------+---------+\n",
      "|            good|  6975703|\n",
      "+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check if the date is ever null - no nulls!\n",
    "spark.sql('''\n",
    "          SELECT DISTINCT\n",
    "              CASE\n",
    "                  WHEN Date IS NULL THEN 'bad_null'\n",
    "                  WHEN Date IS NOT NULL THEN 'good'\n",
    "                  ELSE 'weird'\n",
    "              END AS Date_null_status,    \n",
    "              COUNT(*) as row_count\n",
    "          FROM crime_small          \n",
    "          GROUP BY 1\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IUCR -- check for null values\n",
    "* no null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+\n",
      "|IUCR_null_status|row_count|\n",
      "+----------------+---------+\n",
      "|            good|  6975703|\n",
      "+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''\n",
    "          SELECT DISTINCT\n",
    "              CASE\n",
    "                  WHEN IUCR IS NULL THEN 'bad_null'\n",
    "                  WHEN IUCR IS NOT NULL THEN 'good'\n",
    "                  ELSE 'weird'\n",
    "              END AS IUCR_null_status,    \n",
    "              COUNT(*) as row_count\n",
    "          FROM crime_small          \n",
    "          GROUP BY 1\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Community Area -- check for null values\n",
    "* Community Area is null 8.8% of the time\n",
    "* Null values will be included in fact table\n",
    "* Null can easily be excluded when performing analyses at the community level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|CommArea_null_status|row_count|\n",
      "+--------------------+---------+\n",
      "|                good|  6362209|\n",
      "|            bad_null|   613494|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check if community area is ever null\n",
    "# community area is null a lot, 8.8% of the time -- \n",
    "# I want to be able to evalutate community areas, so it determined that we \n",
    "# still have plenty of data to pick up on seasonal trends\n",
    "spark.sql('''\n",
    "          SELECT DISTINCT\n",
    "              CASE\n",
    "                  WHEN `Community Area` IS NULL THEN 'bad_null'\n",
    "                  WHEN `Community Area` IS NOT NULL THEN 'good'\n",
    "                  ELSE 'weird'\n",
    "              END AS CommArea_null_status,    \n",
    "              COUNT(*) as row_count\n",
    "          FROM crime_small          \n",
    "          GROUP BY 1\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Domestic -- check for null values\n",
    "* no null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|Domestic_null_status|row_count|\n",
      "+--------------------+---------+\n",
      "|                good|  6975703|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check if Domestic is ever null - Domestic is never null!\n",
    "spark.sql('''\n",
    "          SELECT DISTINCT\n",
    "              CASE\n",
    "                  WHEN Domestic IS NULL THEN 'bad_null'\n",
    "                  WHEN Domestic IS NOT NULL THEN 'good'\n",
    "                  ELSE 'weird'\n",
    "              END AS Domestic_null_status,    \n",
    "              COUNT(*) as row_count\n",
    "          FROM crime_small          \n",
    "          GROUP BY 1\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing the data and schema of the weather data, I am primarying interested in the following variables, and will need to perform the following data wrangling steps:\n",
    "* DATE\n",
    "    - Verify the is the primary key of the dataset. \n",
    "* PRCP\n",
    "    - precipitation (inches)\n",
    "    - column should never be null\n",
    "* SNOW\n",
    "    - snowfall (inches)\n",
    "    - columm should never be null\n",
    "* SNWD\n",
    "    - snow depth (inches)\n",
    "    - coalesce() null values to zero\n",
    "* TMAX\n",
    "    - max daily temp (F)\n",
    "    - column should never be null\n",
    "* TMIN\n",
    "    - TMIN - min daily temp (F)\n",
    "    - column should never be null    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data and show the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "weather_raw = spark.read.csv(\"s3a://chicago-crime-ddd/chicago-weather.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `weather_raw` Spark SQL View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather df\n",
    "# 6,836\n",
    "weather_raw.createOrReplaceTempView(\"weather_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only most pertinent columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+----+----+----+----+\n",
      "|      DATE|PRCP|SNOW|SNWD|TMAX|TMIN|TSUN|\n",
      "+----------+----+----+----+----+----+----+\n",
      "|2001-01-01|0.00| 0.0|17.0|  24|   5|null|\n",
      "|2001-01-02|0.00| 0.0|15.0|  19|   5|null|\n",
      "|2001-01-03|0.00| 0.0|14.0|  28|   7|null|\n",
      "|2001-01-04|0.00| 0.0|14.0|  30|  19|null|\n",
      "|2001-01-05|0.00| 0.0|13.0|  36|  21|null|\n",
      "+----------+----+----+----+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_small = spark.sql('''\n",
    "                          SELECT DISTINCT\n",
    "                              DATE,\n",
    "                              PRCP,     \n",
    "                              SNOW,     \n",
    "                              SNWD,     \n",
    "                              TMAX,     \n",
    "                              TMIN,    \n",
    "                              TSUN                           \n",
    "                          FROM weather_raw                          \n",
    "                          ORDER BY DATE\n",
    "                          ''')\n",
    "\n",
    "weather_small.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- PRCP: string (nullable = true)\n",
      " |-- SNOW: string (nullable = true)\n",
      " |-- SNWD: string (nullable = true)\n",
      " |-- TMAX: string (nullable = true)\n",
      " |-- TMIN: string (nullable = true)\n",
      " |-- TSUN: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the schema\n",
    "weather_small.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `weather_small` Spark SQL View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create view\n",
    "weather_small.createOrReplaceTempView(\"weather_small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine Primary Key\n",
    "The primary key should be the DATE column\n",
    "* Yes, the DATE column is the primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+----------------+\n",
      "|row_count|DATE_dist_cnt|STATION_dist_cnt|\n",
      "+---------+-------------+----------------+\n",
      "|     6836|         6836|               1|\n",
      "+---------+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check that row count is the DATE column is indeed the Primary Key\n",
    "spark.sql('''  \n",
    "          SELECT COUNT(*) AS row_count,\n",
    "                 COUNT(distinct DATE) as DATE_dist_cnt,\n",
    "                 COUNT(distinct STATION) as STATION_dist_cnt\n",
    "          FROM weather_raw\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify date range is the same at the crime data\n",
    "* Yes, the date range in the same as the crime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|  min_date|  max_date|\n",
      "+----------+----------+\n",
      "|2001-01-01|2019-09-19|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''  \n",
    "          SELECT DISTINCT\n",
    "              MIN(DATE) AS min_date,\n",
    "              MAX(DATE) AS max_date\n",
    "          FROM weather_raw\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRCP -- check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+\n",
      "|PRCP_null_status|row_count|\n",
      "+----------------+---------+\n",
      "|            good|     6836|\n",
      "+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# null values in my variables of interest\n",
    "\n",
    "# PRCP - no nulls\n",
    "spark.sql('''\n",
    "          SELECT DISTINCT\n",
    "              CASE\n",
    "                  WHEN PRCP IS NULL THEN 'bad_null'\n",
    "                  WHEN PRCP IS NOT NULL THEN 'good'\n",
    "                  ELSE 'weird'\n",
    "              END AS PRCP_null_status,    \n",
    "              COUNT(*) as row_count\n",
    "          FROM weather_small          \n",
    "          GROUP BY 1\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNOW -- check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+\n",
      "|SNOW_null_status|row_count|\n",
      "+----------------+---------+\n",
      "|            good|     6836|\n",
      "+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SNOW\n",
    "spark.sql('''\n",
    "          SELECT DISTINCT\n",
    "              CASE\n",
    "                  WHEN SNOW IS NULL THEN 'bad_null'\n",
    "                  WHEN SNOW IS NOT NULL THEN 'good'\n",
    "                  ELSE 'weird'\n",
    "              END AS SNOW_null_status,    \n",
    "              COUNT(*) as row_count\n",
    "          FROM weather_small          \n",
    "          GROUP BY 1\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNWD -- check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+\n",
      "|SNWD_null_status|row_count|\n",
      "+----------------+---------+\n",
      "|        bad_null|       31|\n",
      "|            good|     6805|\n",
      "+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SNWD - colese nulls to zero\n",
    "# test the nullable false thing here\n",
    "spark.sql('''\n",
    "          SELECT DISTINCT\n",
    "              CASE\n",
    "                  WHEN SNWD IS NULL THEN 'bad_null'\n",
    "                  WHEN SNWD IS NOT NULL THEN 'good'\n",
    "                  ELSE 'weird'\n",
    "              END AS SNWD_null_status,    \n",
    "              COUNT(*) as row_count\n",
    "          FROM weather_small          \n",
    "          GROUP BY 1\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing coalesce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing null values with zeros\n",
    "snow_col = spark.sql('''\n",
    "                     SELECT DISTINCT\n",
    "                         DATE,\n",
    "                         coalesce(SNWD, 0.0) as good_snwd\n",
    "                     FROM weather_small          \n",
    "                     ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark sql view\n",
    "snow_col.createOrReplaceTempView(\"snow_col\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------+\n",
      "|good_snwd_null_status|row_count|\n",
      "+---------------------+---------+\n",
      "|                 good|     6836|\n",
      "+---------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make sure the nulls are gone - they are gone!\n",
    "spark.sql('''\n",
    "          SELECT DISTINCT\n",
    "              CASE\n",
    "                  WHEN good_snwd IS NULL THEN 'bad_null'\n",
    "                  WHEN good_snwd IS NOT NULL THEN 'good'\n",
    "                  ELSE 'weird'\n",
    "              END AS good_snwd_null_status,    \n",
    "              COUNT(*) as row_count\n",
    "          FROM snow_col          \n",
    "          GROUP BY 1\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TMAX -- check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+\n",
      "|TMAX_null_status|row_count|\n",
      "+----------------+---------+\n",
      "|            good|     6836|\n",
      "+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TMAX\n",
    "spark.sql('''\n",
    "          SELECT DISTINCT\n",
    "              CASE\n",
    "                  WHEN TMAX IS NULL THEN 'bad_null'\n",
    "                  WHEN TMAX IS NOT NULL THEN 'good'\n",
    "                  ELSE 'weird'\n",
    "              END AS TMAX_null_status,    \n",
    "              COUNT(*) as row_count\n",
    "          FROM weather_small          \n",
    "          GROUP BY 1\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TMIN -- check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+\n",
      "|TMIN_null_status|row_count|\n",
      "+----------------+---------+\n",
      "|            good|     6836|\n",
      "+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TMIN\n",
    "spark.sql('''\n",
    "          SELECT DISTINCT\n",
    "              CASE\n",
    "                  WHEN TMIN IS NULL THEN 'bad_null'\n",
    "                  WHEN TMIN IS NOT NULL THEN 'good'\n",
    "                  ELSE 'weird'\n",
    "              END AS TMIN_null_status,    \n",
    "              COUNT(*) as row_count\n",
    "          FROM weather_small          \n",
    "          GROUP BY 1\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TSUN -- check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+\n",
      "|TSUN_null_status|row_count|\n",
      "+----------------+---------+\n",
      "|        bad_null|     6101|\n",
      "|            good|      735|\n",
      "+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TSUN - a lot of nulls, so I'm going to throw this out\n",
    "spark.sql('''\n",
    "          SELECT DISTINCT\n",
    "              CASE\n",
    "                  WHEN TSUN IS NULL THEN 'bad_null'\n",
    "                  WHEN TSUN IS NOT NULL THEN 'good'\n",
    "                  ELSE 'weird'\n",
    "              END AS TSUN_null_status,    \n",
    "              COUNT(*) as row_count\n",
    "          FROM weather_small          \n",
    "          GROUP BY 1\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IUCR Data\n",
    "\n",
    "After reviewing the data and schema of the IUCR data, I am primarying interested in the following variables, and will need to perform the following data wrangling steps:\n",
    "* IUCR\n",
    "    - Verify the is the primary key of the dataset. \n",
    "* PRIMARY DESCRIPTION / SECONDARY DESCRIPTION\n",
    "    - Verify that the combination of these two fields is unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "IUCR_raw = spark.read.csv(\"s3a://chicago-crime-ddd/IUCR_Codes.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- IUCR: string (nullable = true)\n",
      " |-- PRIMARY DESCRIPTION: string (nullable = true)\n",
      " |-- SECONDARY DESCRIPTION: string (nullable = true)\n",
      " |-- INDEX CODE: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "IUCR_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "IUCR_raw.createOrReplaceTempView(\"IUCR_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-------------+\n",
      "|row_count|IUCR_dist_cnt|PDSD_dist_cnt|\n",
      "+---------+-------------+-------------+\n",
      "|      401|          401|          401|\n",
      "+---------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''  \n",
    "          SELECT COUNT(*) AS row_count,\n",
    "                 COUNT(distinct IUCR) as IUCR_dist_cnt,\n",
    "                 COUNT(distinct `PRIMARY DESCRIPTION` || `SECONDARY DESCRIPTION`) as PDSD_dist_cnt\n",
    "          FROM IUCR_raw\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Data Model\n",
    "#### Schema\n",
    "\n",
    "`crime_fact`\n",
    "* primary key: date, IUCR, domestic, community_area\n",
    "- type: fact table\n",
    "    - col: date\n",
    "    - col: year\n",
    "    - col: month\n",
    "    - col: IUCR\n",
    "    - col: domestic\n",
    "    - col: community_area\n",
    "    - col: crime_count\n",
    "\n",
    "`weather_dim`\n",
    "* primary key: DATE\n",
    "* type: dimension table\n",
    "    - col: DATE\n",
    "    - col: PRCP\n",
    "    - col: SNOW\n",
    "    - col: SNWD\n",
    "    - col: TMAX\n",
    "    - col: TMIN\n",
    "\n",
    "`iucr_dim`\n",
    "* primary key: IUCR code\n",
    "* type: dimension table\n",
    "    - col: IUCR\n",
    "    - col: primary_description\n",
    "    - col: secondary_description\n",
    "\n",
    "\n",
    "\n",
    "#### Mapping of Data Pipeline\n",
    "1. Load CSVs from s3\n",
    "2. Create Spark SQL views\n",
    "3. Create Fact and Dimension tables\n",
    "    - Crime fact\n",
    "    - Weather dim\n",
    "    - IUCR dim\n",
    "4. Perform data quality checks\n",
    "    - test data load row counts\n",
    "    - check for null values\n",
    "    - test primary keys in both raw uploads and clean tables\n",
    "5. Write as parquet files back to s3     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load CSVs from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_raw = spark.read.csv(\"s3a://chicago-crime-ddd/Chicago_Crime_2001_to_Present.csv\", header=True)\n",
    "weather_raw = spark.read.csv(\"s3a://chicago-crime-ddd/chicago-weather.csv\", header=True)\n",
    "IUCR_raw = spark.read.csv(\"s3a://chicago-crime-ddd/IUCR_Codes.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Spark SQL views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_raw.createOrReplaceTempView(\"crime_raw\")\n",
    "weather_raw.createOrReplaceTempView(\"weather_raw\")\n",
    "IUCR_raw.createOrReplaceTempView(\"IUCR_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create fact and dimension tables \n",
    "##### Create CRIME fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_fact = spark.sql('''\n",
    "                       SELECT DISTINCT\n",
    "                           to_date(concat(substr(Date, 7, 4), '-', substr(Date, 1, 2), '-', substr(Date, 4, 2)), 'yyyy-MM-dd') AS date,\n",
    "                           CAST(TRIM(Year) AS string) AS year,\n",
    "                           lpad(CAST(month(to_date(concat(substr(Date, 7, 4), '-', substr(Date, 1, 2), '-', substr(Date, 4, 2)), 'yyyy-MM-dd')) AS string), 2, '0') AS month,\n",
    "                           CAST(TRIM(IUCR) AS string) AS IUCR,\n",
    "                           CAST(TRIM(Domestic) AS string) AS domestic,\n",
    "                           CAST(TRIM(coalesce(`Community Area`, 'no_community_reported')) AS string) AS community_area,\n",
    "                           COUNT(distinct ID) as crime_count\n",
    "                       FROM crime_raw          \n",
    "                       WHERE\n",
    "                           `Case Number` IS NOT NULL\n",
    "                           AND\n",
    "                           `Community Area` IS NOT NULL\n",
    "                       GROUP BY 1,2,3,4,5,6\n",
    "                       ORDER BY date\n",
    "                       ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create WEATHER dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dim = spark.sql('''\n",
    "                        SELECT\n",
    "                            to_date(DATE, 'yyyy-MM-dd') AS DATE,\n",
    "                            CAST(PRCP AS FLOAT) AS PRCP,  \n",
    "                            CAST(SNOW AS FLOAT) AS SNOW,                                    \n",
    "                            CAST(coalesce(SNWD, 0.0) AS FLOAT) AS SNWD,\n",
    "                            CAST(TMAX AS INT) AS TMAX,  \n",
    "                            CAST(TMIN AS INT) AS TMIN\n",
    "                        FROM weather_raw          \n",
    "                        ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create IUCR dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "IUCR_dim = spark.sql('''\n",
    "                     SELECT\n",
    "                         CAST(TRIM(IUCR) AS string) AS IUCR,\n",
    "                         CAST(TRIM(`PRIMARY DESCRIPTION`) AS string) AS primary_description,\n",
    "                         CAST(TRIM(`SECONDARY DESCRIPTION`) AS string) AS secondary_description\n",
    "                     FROM IUCR_raw          \n",
    "                     ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Data Qualty Checks\n",
    "##### Test that data loads match expected values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_row_count_check(df, target_row_count):    \n",
    "    row_count = df.count()\n",
    "    \n",
    "    if row_count != target_row_count:\n",
    "        print(f\"Upload row count test fail. Row count does not match expected number of rows\")\n",
    "    else:\n",
    "        print(f\"Test passed! Upload was sucessfull\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crime_raw row count test\n",
      "Test passed! Upload was sucessfull\n",
      "weather_raw row count test\n",
      "Test passed! Upload was sucessfull\n",
      "IUCR_raw row count test\n",
      "Test passed! Upload was sucessfull\n"
     ]
    }
   ],
   "source": [
    "print(\"crime_raw row count test\")\n",
    "upload_row_count_check(crime_raw, 6975703)\n",
    "\n",
    "print(\"weather_raw row count test\")\n",
    "upload_row_count_check(weather_raw, 6836)\n",
    "\n",
    "print(\"IUCR_raw row count test\")\n",
    "upload_row_count_check(IUCR_raw, 401)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test that variables of interest do not contain unsuspected null values in data uploads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_col_for_nulls(df, column_name, threshold=1, stat='identity'):\n",
    "        \n",
    "    null_count = df.where(col(column_name).isNull()).count()\n",
    "    row_count = df.count()\n",
    "    percent_null = ((null_count / row_count) * 100.0)\n",
    "    \n",
    "    print(f\"----null column check / {column_name}----\")\n",
    "    print(f\"null count: {null_count}\")\n",
    "    print(f\"row count: {row_count}\")\n",
    "    print(f\"percent nulls in column: {percent_null}\")\n",
    "    print(f\"threshold: {threshold}\")\n",
    "    print(f\"stat: {stat}\")\n",
    "    \n",
    "    if stat == 'identity':\n",
    "        if null_count >= threshold:\n",
    "            print(\"Test failed: null threshold met or supassed\")\n",
    "        else:\n",
    "            print(\"Null test passed\")\n",
    "            \n",
    "    if stat == 'percent':\n",
    "        if percent_null >= threshold:\n",
    "            print(\"Test failed: threshold met or supassed\")\n",
    "        else:\n",
    "            print(\"Null test passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crime_raw null value checks\n",
      "----null column check / Date----\n",
      "null count: 0\n",
      "row count: 6975703\n",
      "percent nulls in column: 0.0\n",
      "threshold: 1\n",
      "stat: identity\n",
      "Null test passed\n",
      "----null column check / IUCR----\n",
      "null count: 0\n",
      "row count: 6975703\n",
      "percent nulls in column: 0.0\n",
      "threshold: 1\n",
      "stat: identity\n",
      "Null test passed\n",
      "----null column check / ID----\n",
      "null count: 0\n",
      "row count: 6975703\n",
      "percent nulls in column: 0.0\n",
      "threshold: 1\n",
      "stat: identity\n",
      "Null test passed\n",
      "----null column check / `Community Area`----\n",
      "null count: 613494\n",
      "row count: 6975703\n",
      "percent nulls in column: 8.794726495666458\n",
      "threshold: 10\n",
      "stat: percent\n",
      "Null test passed\n"
     ]
    }
   ],
   "source": [
    "print(\"crime_raw null value checks\")\n",
    "check_col_for_nulls(crime_raw, 'Date', threshold=1, stat='identity')\n",
    "check_col_for_nulls(crime_raw, 'IUCR', threshold=1, stat='identity')\n",
    "check_col_for_nulls(crime_raw, 'ID', threshold=1, stat='identity')\n",
    "check_col_for_nulls(crime_raw, '`Community Area`', threshold=10, stat='percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weather_raw null value checks\n",
      "----null column check / DATE----\n",
      "null count: 0\n",
      "row count: 6836\n",
      "percent nulls in column: 0.0\n",
      "threshold: 1\n",
      "stat: identity\n",
      "Null test passed\n",
      "----null column check / PRCP----\n",
      "null count: 0\n",
      "row count: 6836\n",
      "percent nulls in column: 0.0\n",
      "threshold: 1\n",
      "stat: identity\n",
      "Null test passed\n",
      "----null column check / SNOW----\n",
      "null count: 0\n",
      "row count: 6836\n",
      "percent nulls in column: 0.0\n",
      "threshold: 1\n",
      "stat: identity\n",
      "Null test passed\n",
      "----null column check / SNWD----\n",
      "null count: 31\n",
      "row count: 6836\n",
      "percent nulls in column: 0.45348156816851964\n",
      "threshold: 1\n",
      "stat: percent\n",
      "Null test passed\n",
      "----null column check / TMAX----\n",
      "null count: 0\n",
      "row count: 6836\n",
      "percent nulls in column: 0.0\n",
      "threshold: 1\n",
      "stat: identity\n",
      "Null test passed\n",
      "----null column check / TMIN----\n",
      "null count: 0\n",
      "row count: 6836\n",
      "percent nulls in column: 0.0\n",
      "threshold: 1\n",
      "stat: identity\n",
      "Null test passed\n"
     ]
    }
   ],
   "source": [
    "print(\"weather_raw null value checks\")\n",
    "check_col_for_nulls(weather_raw, 'DATE', threshold=1, stat='identity')\n",
    "check_col_for_nulls(weather_raw, 'PRCP', threshold=1, stat='identity')\n",
    "check_col_for_nulls(weather_raw, 'SNOW', threshold=1, stat='identity')\n",
    "check_col_for_nulls(weather_raw, 'SNWD', threshold=1, stat='percent')\n",
    "check_col_for_nulls(weather_raw, 'TMAX', threshold=1, stat='identity')\n",
    "check_col_for_nulls(weather_raw, 'TMIN', threshold=1, stat='identity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IUCR_raw null value checks\n",
      "----null column check / IUCR----\n",
      "null count: 0\n",
      "row count: 401\n",
      "percent nulls in column: 0.0\n",
      "threshold: 1\n",
      "stat: identity\n",
      "Null test passed\n",
      "----null column check / `PRIMARY DESCRIPTION`----\n",
      "null count: 0\n",
      "row count: 401\n",
      "percent nulls in column: 0.0\n",
      "threshold: 1\n",
      "stat: identity\n",
      "Null test passed\n",
      "----null column check / `SECONDARY DESCRIPTION`----\n",
      "null count: 0\n",
      "row count: 401\n",
      "percent nulls in column: 0.0\n",
      "threshold: 1\n",
      "stat: identity\n",
      "Null test passed\n"
     ]
    }
   ],
   "source": [
    "print(\"IUCR_raw null value checks\")\n",
    "check_col_for_nulls(IUCR_raw, 'IUCR', threshold=1, stat='identity')\n",
    "check_col_for_nulls(IUCR_raw, '`PRIMARY DESCRIPTION`', threshold=1, stat='identity')\n",
    "check_col_for_nulls(IUCR_raw, '`SECONDARY DESCRIPTION`', threshold=1, stat='identity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Primary Keys of raw and cleaned tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_primary_key(df, pk_column):\n",
    "    \n",
    "    row_count = df.count()\n",
    "    pk_test_count = df.select(pk_column).distinct().count()\n",
    "    \n",
    "    if pk_test_count != row_count:\n",
    "        print(f\"Primary key test failed. {pk_column} is not the primary key of this table\")\n",
    "    else:\n",
    "        print(f\"Test passed! {pk_column} is the primary key of this table\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test PK of raw tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crime_raw pk test\n",
      "Test passed! ID is the primary key of this table\n",
      "weather_raw pk test\n",
      "Test passed! DATE is the primary key of this table\n",
      "IUCR_raw pk test\n",
      "Test passed! IUCR is the primary key of this table\n"
     ]
    }
   ],
   "source": [
    "print(\"crime_raw pk test\")\n",
    "test_primary_key(crime_raw, 'ID')\n",
    "\n",
    "print(\"weather_raw pk test\")\n",
    "test_primary_key(weather_raw, 'DATE')\n",
    "\n",
    "print(\"IUCR_raw pk test\")\n",
    "test_primary_key(IUCR_raw, 'IUCR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test PK of final fact and dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weather_raw pk test\n",
      "Test passed! DATE is the primary key of this table\n",
      "IUCR_raw pk test\n",
      "Test passed! IUCR is the primary key of this table\n",
      "crime_fact pk test\n",
      "+---------+-------------------+\n",
      "|row_count|crime_composite_key|\n",
      "+---------+-------------------+\n",
      "|  4502692|            4502692|\n",
      "+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"weather_raw pk test\")\n",
    "test_primary_key(weather_raw, 'DATE')\n",
    "\n",
    "print(\"IUCR_raw pk test\")\n",
    "test_primary_key(IUCR_raw, 'IUCR')\n",
    "\n",
    "# manual test for crime_fact\n",
    "print(\"crime_fact pk test\")\n",
    "crime_fact.createOrReplaceTempView(\"crime_fact\")\n",
    "spark.sql('''  \n",
    "          SELECT DISTINCT\n",
    "                 COUNT(*) AS row_count,                 \n",
    "                 COUNT(distinct date || IUCR || domestic || community_area) as crime_composite_key\n",
    "          FROM crime_fact\n",
    "          ''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write tables to parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Spark SQL tables to Pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_fact = crime_fact.toPandas()\n",
    "weather_dim = weather_dim.toPandas()\n",
    "IUCR_dim = IUCR_dim.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write tables to s3 as Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_fact.write.partitionBy('year').parquet(os.path.join(\"s3a://chicago-crime-ddd/\", 'crime_fact'), 'overwrite')\n",
    "weather_dim.write.parquet(os.path.join(\"s3a://chicago-crime-ddd/\", 'weather_dim'), 'overwrite')\n",
    "IUCR_dim.write.parquet(os.path.join(\"s3a://chicago-crime-ddd/\", 'IUCR_dim'), 'overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data dictionary \n",
    "\n",
    "`crime_fact`\n",
    "* date\n",
    "    - Day crime occured\n",
    "* year\n",
    "    - Year crime occured\n",
    "* month\n",
    "    - Month crime occured\n",
    "* IUCR\n",
    "    - Illinois Unifrom Crime Reporting code. This is directly linked to the Primary Type and Description\n",
    "* domestic\n",
    "    - Indicates whether the incident was domestic-related as defined by the Illinois Domestic Violence Act \n",
    "* community_area\n",
    "    - Indicates the community area where the incident occurred. Chicago has 77 community areas.\n",
    "* crime_count\n",
    "    - Count of crimes at the day, IUCR, domestic, community area level\n",
    "\n",
    "`weather_dim`\n",
    "* DATE\n",
    "    - Day of weather statistics \n",
    "* PRCP\n",
    "    - Precipitation (inches)\n",
    "* SNOW\n",
    "    - Snowfall (inches)\n",
    "* SNWD\n",
    "    - Snow depth (inches)\n",
    "* TMAX\n",
    "    - Max daily temp (F)\n",
    "* TMIN\n",
    "    - Min daily temp (F)\n",
    "\n",
    "`iucr_dim`\n",
    "* IUCR\n",
    "    - Illinois Unifrom Crime Reporting code\n",
    "* primary_description\n",
    "    - Main description of crime. Example: Homicide \n",
    "* secondary_description\n",
    "    - Secondary descrition of crime. Example: Aggravated: Handgun "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Write Up\n",
    "\n",
    "* *Clearly state the rationale for the choice of tools and technologies for the project.*\n",
    "    - Spark was chosen because the primary aim of the project is the clean, verify, and store the data in a format that allows data scientists and data analyst the ability to quickly load the tables and run timeseries and machine learning algorthms.\n",
    "    - By using the Spark and s3, a data scientist will only have to load the parquet files, which only takes a few seconds, and then they can immediatly run timeseries and machine learning algorthms in the same environment.\n",
    "    - Because the primary aim is to run statistical models, and not to have the data availble to perform descriptive, SQL analyses -- writing the data out as a parquet file was chosen over loading it into a database. Writing the data out as parquet files and storing it on s3 is also a lot cheaper than storing it in a Postgres or Redshift database.     \n",
    "\n",
    "\n",
    "* *Propose how often the data should be updated and why.*\n",
    "    - The data should be updated monthly. If there are seasonal patterns, they will month likely be detected at the month level. \n",
    "    - Month level aggregations will be frequent. Therefore, it is important that data is not upload until there is a complete month's of data to upload, as partial months could lead to deceptive results. \n",
    "\n",
    "\n",
    "* *Write a description of how you would approach the problem differently under the following scenarios:*\n",
    " * The data was increased by 100x.\n",
    "     - Since I am in spark, I am good to go. As the data increases, I can just use more machines and pool more memory. Especially since the primary goal is to run machine learning learning algorthms.\n",
    "\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "     - In this situation, it would be best to template my data pipeline using Airflow. By using Airflow, I can easily schedule and monitor my pipeline. It will also be really easy to check the status of the pipeline through Airflow's GUI and to quickly detech errors. \n",
    " * The database needed to be accessed by 100+ people.\n",
    "     - If the database needed to be accessed by 100+ people, then a true database would need to be created. Since the purpose of this data piple line is to perform data analysis, the two best choices would either be Postgres or Redshift. Postgres is a cheaper and more straightforward option, and with the data at its current size, would be plenty efficient. But even with the data at its current size, Redshift could potentially be more performant. And as the data continues to grow, if say the db is around for the next 10 years, then using a distributed SQL framwork like Redshift is the right tool for the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
